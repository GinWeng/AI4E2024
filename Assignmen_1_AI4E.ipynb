{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+9ZL4KW1TZc9ZFIsYCyKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GinWeng/AI4E2024/blob/main/Assignmen_1_AI4E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I: Distance Functions"
      ],
      "metadata": {
        "id": "I94dHjYXRily"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Documentation: Euclidean Distance Calculation</summary>\n",
        "\n",
        "  ### Study Progress Documentation: Euclidean Distance Function Development\n",
        "  #### Date\n",
        "  April 01, 2025\n",
        "\n",
        "  #### Project Context\n",
        "  This documentation pertains to a project task within the subject \"Artificial Intelligence for Engineering.\" The task requires writing a Python function to calculate the Euclidean distance between two n-dimensional data points, contributing 0.25 points to the project grade. The function assumes that input vectors are of equal length, as specified in the project description.\n",
        "\n",
        "  #### Concept Overview\n",
        "  The Euclidean distance represents the straight-line distance between two points in n-dimensional space. It is a fundamental metric in fields such as machine learning and engineering, often used to measure similarity or dissimilarity between data points. Mathematically, for two n-dimensional vectors \\( A = (a_1, a_2, ..., a_n) \\) and \\( B = (b_1, b_2, ..., b_n) \\), the Euclidean distance is defined as:\n",
        "\n",
        "  \\[\n",
        "  d_{Euclidean} = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n",
        "  \\]\n",
        "\n",
        "  This formula generalizes the Pythagorean theorem from two dimensions to any number of dimensions, computing the square root of the sum of squared differences between corresponding components.\n",
        "\n",
        "  #### Development Process\n",
        "  Our exploration began with the 2D case, where the distance between points \\( A = (x_A, y_A) \\) and \\( B = (x_B, y_B) \\) was expressed as:\n",
        "\n",
        "  \\[\n",
        "  d_{Euclidean} = \\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2}\n",
        "  \\]\n",
        "\n",
        "  Recognizing the pattern, we extended this to n dimensions, arriving at the generalized formula above. We then translated this into a Python implementation, iterating through several versions to optimize clarity and efficiency.\n",
        "\n",
        "  #### Initial Python Implementation\n",
        "  The first attempt used a manual loop to compute the sum of squared differences:\n",
        "  ```python\n",
        "  def d_euc(A, B):\n",
        "      assert len(A) == len(B)  # Ensure vectors are of equal length\n",
        "      distance = 0\n",
        "      for i in range(len(A)):\n",
        "          distance += (A[i] - B[i])**2\n",
        "      return np.sqrt(distance)\n",
        "  ```\n",
        "  - **Strengths**: Intuitive and closely mirrors the mathematical formula.\n",
        "  - **Considerations**: Assumes NumPy is available for `np.sqrt()` but lacks an explicit import. Input type flexibility (e.g., lists or arrays) was not explicitly handled.\n",
        "\n",
        "  #### Refined Implementation\n",
        "  To leverage NumPy’s efficiency and ensure robustness, we refined the function:\n",
        "  ```python\n",
        "  import numpy as np\n",
        "  def d_euc(A, B):\n",
        "      A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "      B = np.array(B)\n",
        "      assert A.size == B.size  # Verify equal lengths\n",
        "      distance = np.linalg.norm((A - B))  # Compute Euclidean norm\n",
        "      return distance\n",
        "  ```\n",
        "  - **Key Improvements**:\n",
        "    - Added `import numpy as np` for clarity and dependency transparency.\n",
        "    - Converted inputs to NumPy arrays, ensuring compatibility with various input types (e.g., lists, tuples).\n",
        "    - Replaced the loop with `np.linalg.norm()`, which computes the L2 (Euclidean) norm directly, eliminating the need for an additional `np.sqrt()`.\n",
        "    - Used `A.size` instead of `len(A)` to align with NumPy array properties.\n",
        "\n",
        "  #### How It Works\n",
        "  - **Input**: Two vectors \\( A \\) and \\( B \\), each with n components (e.g., \\( A = [1, 2, 3] \\), \\( B = [4, 5, 6] \\)).\n",
        "  - **Steps**:\n",
        "    1. Convert \\( A \\) and \\( B \\) to NumPy arrays.\n",
        "    2. Check that their sizes match using `assert A.size == B.size`.\n",
        "    3. Compute the difference vector \\( A - B \\) (e.g., \\( [1-4, 2-5, 3-6] = [-3, -3, -3] \\)).\n",
        "    4. Use `np.linalg.norm()` to calculate the Euclidean norm of the difference vector (e.g., \\( \\sqrt{(-3)^2 + (-3)^2 + (-3)^2} = \\sqrt{27} \\approx 5.196 \\)).\n",
        "    5. Return the result.\n",
        "\n",
        "  #### Example\n",
        "  For \\( A = [1, 2, 3] \\) and \\( B = [4, 5, 6] \\):\n",
        "  - \\( A - B = [-3, -3, -3] \\)\n",
        "  - \\( d_{Euclidean} = \\sqrt{(-3)^2 + (-3)^2 + (-3)^2} = \\sqrt{9 + 9 + 9} = \\sqrt{27} \\approx 5.196 \\)\n",
        "  - Function output: `5.196152422706632`\n",
        "\n",
        "  #### Final Notes\n",
        "  This implementation is concise, efficient, and robust, leveraging NumPy for vectorized operations. It meets the project requirements by handling n-dimensional vectors of equal length and provides accurate Euclidean distance calculations. Potential further refinements could include eliminating the `distance` variable for brevity (e.g., `return np.linalg.norm(A - B)`), though the current form balances readability and functionality.\n",
        "\n",
        "  ---\n",
        "</details>"
      ],
      "metadata": {
        "id": "-h-b0Tmu0y2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6yR6dFaIhSv"
      },
      "outputs": [],
      "source": [
        "# Euclidean Distance\n",
        "import numpy as np\n",
        "def d_euc(A, B):\n",
        "    A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "    B = np.array(B)\n",
        "    assert A.size == B.size  # Verify equal lengths\n",
        "    distance = np.linalg.norm((A - B))  # Compute Euclidean norm\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary> Manhattan Distance</summary>\n",
        "\n",
        "  ### Study Progress Documentation: Manhattan Distance Function Development\n",
        "\n",
        "  #### Date\n",
        "  April 01, 2025\n",
        "\n",
        "  #### Project Context\n",
        "  This documentation relates to a project task within the subject \"Artificial Intelligence for Engineering.\" The task requires writing a Python function to calculate the Manhattan distance between two n-dimensional data points, contributing 0.25 points to the project grade. The function assumes that input vectors are of equal length, as specified in the project description.\n",
        "\n",
        "  #### Concept Overview\n",
        "  The Manhattan distance, also known as the \"taxicab distance\" or L1 norm, measures the distance between two points in n-dimensional space by summing the absolute differences of their coordinates. Unlike Euclidean distance, which measures a straight line, Manhattan distance reflects travel along grid-like paths, as if moving only along the axes. For two n-dimensional vectors \\( A = (a_1, a_2, ..., a_n) \\) and \\( B = (b_1, b_2, ..., b_n) \\), the Manhattan distance is defined as:\n",
        "\n",
        "  \\[\n",
        "  d_{Manhattan} = \\sum_{i=1}^{n} |a_i - b_i|\n",
        "  \\]\n",
        "\n",
        "  This metric is widely used in machine learning and data analysis, particularly when differences in individual dimensions are more meaningful than overall geometric distance.\n",
        "\n",
        "  #### Development Process\n",
        "  We started by imagining a grid-like city layout, where travel between points \\( A = (1, 2) \\) and \\( B = (3, 5) \\) occurs along horizontal and vertical paths. This led to the realization that the distance is the sum of absolute differences in each dimension, which we generalized to n dimensions. We then crafted a Python implementation, refining it for efficiency and clarity.\n",
        "\n",
        "  #### Python Implementation\n",
        "  The final implementation leverages NumPy for vectorized operations:\n",
        "  ```python\n",
        "  import numpy as np\n",
        "  def d_manh(A, B):\n",
        "      A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "      B = np.array(B)\n",
        "      assert A.size == B.size  # Verify equal lengths\n",
        "      distance = np.sum(np.abs(A - B))  # Sum absolute differences\n",
        "      return distance\n",
        "  ```\n",
        "  - **Key Features**:\n",
        "    - Explicitly imports `numpy` as `np` for transparency.\n",
        "    - Converts inputs to NumPy arrays, ensuring flexibility with input types (e.g., lists, tuples).\n",
        "    - Uses `A.size == B.size` to confirm equal vector lengths, consistent with the project’s assumption.\n",
        "    - Employs `np.abs(A - B)` to compute absolute differences and `np.sum()` to add them, avoiding manual loops.\n",
        "    - Retains the `distance` variable for improved readability, a deliberate choice for human understanding.\n",
        "\n",
        "  #### How It Works\n",
        "  - **Input**: Two vectors \\( A \\) and \\( B \\), each with n components (e.g., \\( A = [1, 2] \\), \\( B = [3, 5] \\)).\n",
        "  - **Steps**:\n",
        "    1. Convert \\( A \\) and \\( B \\) to NumPy arrays.\n",
        "    2. Verify equal sizes with `assert A.size == B.size`.\n",
        "    3. Compute the difference vector \\( A - B \\) (e.g., \\( [1 - 3, 2 - 5] = [-2, -3] \\)).\n",
        "    4. Take absolute values: \\( np.abs([-2, -3]) = [2, 3] \\).\n",
        "    5. Sum the absolute differences: \\( np.sum([2, 3]) = 2 + 3 = 5 \\).\n",
        "    6. Return the result.\n",
        "\n",
        "  #### Example\n",
        "  For \\( A = [1, 2] \\) and \\( B = [3, 5] \\):\n",
        "  - \\( A - B = [-2, -3] \\)\n",
        "  - \\( |A - B| = [2, 3] \\)\n",
        "  - \\( d_{Manhattan} = 2 + 3 = 5 \\)\n",
        "  - Function output: `5`\n",
        "\n",
        "  This result corresponds to moving 2 units in the x-direction (\\( |1 - 3| \\)) and 3 units in the y-direction (\\( |2 - 5| \\)), totaling 5 units along the grid.\n",
        "\n",
        "  #### Validation\n",
        "  The result aligns with the conceptual model of \"taxicab\" travel, where diagonal movement is not allowed. The absolute values ensure that distances in each dimension are treated as positive lengths, preventing cancellation of differences, which is a key distinction from Euclidean distance.\n",
        "\n",
        "  #### Final Notes\n",
        "  This implementation is efficient, readable, and robust, meeting the project requirements for n-dimensional vectors of equal length. The use of NumPy’s vectorized operations eliminates the need for explicit loops, while retaining `distance` enhances code clarity. A more concise version could omit `distance` (e.g., `return np.sum(np.abs(A - B))`)), but the current form prioritizes human understanding, as preferred.\n",
        "</details>"
      ],
      "metadata": {
        "id": "2GocV6cu1mYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manhattan Distance\n",
        "import numpy as np\n",
        "def d_manh(A,B):\n",
        "    A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "    B = np.array(B)\n",
        "    assert A.size == B.size  # Verify equal lengths\n",
        "    distance = np.sum(np.abs(A-B))\n",
        "    return distance"
      ],
      "metadata": {
        "id": "fg33bAu41m_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Cosine Distance</summary>\n",
        "---\n",
        "\n",
        "### Study Progress Documentation: Cosine Distance Function Development\n",
        "**Date**: April 02, 2025  \n",
        "**Collaborators**: User, Grok 3 (xAI)\n",
        "\n",
        "#### Objective\n",
        "Develop and refine a Python function `d_cos(A, B)` to compute the cosine distance between two vectors, ensuring robustness, clarity, and alignment with project requirements.\n",
        "\n",
        "#### Initial Implementation\n",
        "The starting point was your cosine distance function:\n",
        "```python\n",
        "import numpy as np\n",
        "def d_cos(A, B):\n",
        "    A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "    B = np.array(B)\n",
        "    assert A.size == B.size  # Verify equal lengths\n",
        "    assert np.linalg.norm(A) != 0 and np.linalg.norm(B) != 0, \"Vectors cannot be zero vectors\"\n",
        "    sim_cos = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B)) # calculate the cosine similarity\n",
        "    distance = 1 - sim_cos\n",
        "    return distance\n",
        "```\n",
        "- **Purpose**: Compute \\( d_{cos} = 1 - \\cos(\\theta) \\), where \\( \\cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||} \\).\n",
        "- **Range**: [0, 2] (0 for identical, 1 for perpendicular, 2 for opposite vectors).\n",
        "- **Features**: Input validation, zero-vector check, NumPy optimization.\n",
        "\n",
        "#### Key Discussion Points\n",
        "1. **Input Handling**  \n",
        "   - Converting inputs to NumPy arrays ensures flexibility (e.g., lists, tuples).  \n",
        "   - `assert A.size == B.size` enforces equal vector lengths, a project assumption.  \n",
        "   - **Outcome**: Retained as robust and appropriate.\n",
        "\n",
        "2. **Zero-Vector Handling**  \n",
        "   - Current: Raises an error if \\( ||A|| = 0 \\) or \\( ||B|| = 0 \\) to avoid division by zero.  \n",
        "   - Alternatives Considered: Return `None`, `float('inf')`, or a default value (e.g., 1).  \n",
        "   - **Rationale**: Error aligns with mathematical correctness and prevents ambiguous results.  \n",
        "   - **Decision**: Kept the assertion, pending your final use-case confirmation.\n",
        "\n",
        "3. **Cosine Similarity and Distance**  \n",
        "   - Formula: \\( \\cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||} \\), \\( d_{cos} = 1 - \\cos(\\theta) \\).  \n",
        "   - Implementation uses `np.dot()` and `np.linalg.norm()`, matching the mathematical definition.  \n",
        "   - Range [0, 2] confirmed via tests. Alternative \\( 1 - |\\cos(\\theta)| \\) for [0, 1] was proposed.  \n",
        "   - **Decision**: Retained [0, 2] range, awaiting your preference.\n",
        "\n",
        "4. **Readability vs. Efficiency**  \n",
        "   - Separate `sim_cos` variable enhances clarity.  \n",
        "   - One-liner option: `return 1 - np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))`.  \n",
        "   - **Outcome**: Kept separate variable for readability, with efficiency tweak below.\n",
        "\n",
        "#### Testing\n",
        "- **Same Direction**: \\( [1, 2] \\), \\( [2, 4] \\) → \\( d_{cos} = 0 \\) (✓).  \n",
        "- **Perpendicular**: \\( [1, 0] \\), \\( [0, 1] \\) → \\( d_{cos} = 1 \\) (✓).  \n",
        "- **Zero Vector**: \\( [0, 0] \\), \\( [1, 2] \\) → Error (✓).  \n",
        "- **Opposite Direction**: \\( [1, 0] \\), \\( [-1, 0] \\) → \\( d_{cos} = 2 \\) (✓).  \n",
        "- **Higher Dimension**: \\( [1, 2, 3] \\), \\( [4, 5, 6] \\) → \\( d_{cos} \\approx 0.027 \\) (✓).  \n",
        "- **Conclusion**: Function performs correctly across cases.\n",
        "\n",
        "#### Refined Implementation\n",
        "```python\n",
        "import numpy as np\n",
        "def d_cos(A, B):\n",
        "    A = np.array(A)\n",
        "    B = np.array(B)\n",
        "    assert A.size == B.size, \"Vectors must have equal lengths\"\n",
        "    norm_A, norm_B = np.linalg.norm(A), np.linalg.norm(B)\n",
        "    assert norm_A != 0 and norm_B != 0, \"Vectors cannot be zero vectors\"\n",
        "    sim_cos = np.dot(A, B) / (norm_A * norm_B)\n",
        "    return 1 - sim_cos\n",
        "```\n",
        "- **Improvements**: Cached norms for minor efficiency; retained readability.  \n",
        "- **Options**: Swap to `1 - abs(sim_cos)` for [0, 1] range if needed.\n",
        "\n",
        "#### Open Questions for Future Reference\n",
        "1. **Zero-Vector Behavior**: Confirm if error is preferred or if a default value suits your project.  \n",
        "2. **Range**: Finalize [0, 2] vs. [0, 1] based on application (e.g., clustering, similarity scoring).  \n",
        "3. **Additional Tests**: Consider negative vectors, sparse inputs, or larger datasets if relevant.\n",
        "\n",
        "#### Next Steps\n",
        "- Awaiting your feedback on open questions.  \n",
        "- Potential: Add docstring, integrate into broader project, or test with real data.\n",
        "\n",
        "#### Notes\n",
        "Your work demonstrates a strong grasp of vector operations and NumPy usage. This function is ready for use or further customization as your project evolves. Feel free to revisit this with specific requirements or new test cases!\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "zabHnoCt3Hcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Distance\n",
        "import numpy as np\n",
        "def d_cos(A,B):\n",
        "    A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "    B = np.array(B)\n",
        "    assert A.size == B.size  # Verify equal lengths\n",
        "\n",
        "    norm_A, norm_B = np.linalg.norm(A), np.linalg.norm(B)\n",
        "    assert norm_A != 0 and norm_B != 0, \"Vectors cannot be zero vectors\"\n",
        "      # check if the length(s) of vector(S) is (are) 0 to avoid 0 denominator\n",
        "\n",
        "    sim_cos = np.dot(A, B) / (norm_A * norm_B) # calculate the cosine similarity\n",
        "    distance = 1 - sim_cos\n",
        "    return distance"
      ],
      "metadata": {
        "id": "YYGoB6Go3HxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Hamming Distance</summary>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Study Progress Documentation: Hamming Distance Function Development\n",
        "**Date**: April 02, 2025  \n",
        "**Collaborators**: User, Grok 3 (xAI)\n",
        "\n",
        "#### Objective\n",
        "Develop a Python function `d_ham(A, B)` to compute the Hamming distance between two binary strings, ensuring it counts positional differences accurately and enforces binary string constraints.\n",
        "\n",
        "#### Initial Understanding\n",
        "You described Hamming distance as “comparing two words digit by digit, only incrementing distance by one if there is any discrepancy,” and for vectors, “comparing difference instead of calculating the actual distance.” This led to your first draft:\n",
        "```python\n",
        "import numpy as np\n",
        "def d_ham(A, B):\n",
        "    A = np.array(A)  # Convert inputs to NumPy arrays\n",
        "    B = np.array(B)\n",
        "    assert A.size == B.size  # Verify equal lengths\n",
        "    distance = np.sum(A != B)\n",
        "    return distance\n",
        "```\n",
        "- **Intent**: Count mismatches between array elements.\n",
        "- **Observation**: Works for arrays (e.g., `[1, 0]` vs. `[1, 1]`), but not directly for strings like `\"101\"` vs. `\"110\"`.\n",
        "\n",
        "#### Key Discussion Points\n",
        "1. **Scope Refinement**  \n",
        "   - You clarified the function should handle *binary strings* (e.g., `\"1010\"`, `\"1100\"`), not general arrays or vectors.  \n",
        "   - **Outcome**: Shifted focus to string inputs with '0' and '1' only.\n",
        "\n",
        "2. **Input Handling**  \n",
        "   - Initial code used NumPy arrays, but `np.array(\"101\")` treats the string as a single element.  \n",
        "   - Explored treating strings as character sequences (e.g., `['1', '0', '1']`).  \n",
        "   - **Question**: How to ensure inputs are binary strings?\n",
        "\n",
        "3. **Validation**  \n",
        "   - Added checks for string type (`isinstance`) and binary content (`all(char in '01' for char in A)`).  \n",
        "   - Kept equal-length requirement, as Hamming distance is undefined otherwise.  \n",
        "   - **Outcome**: Strict input validation to match your binary string goal.\n",
        "\n",
        "4. **Distance Calculation**  \n",
        "   - Initial `np.sum(A != B)` worked for arrays but needed adaptation for strings.  \n",
        "   - Proposed `sum(a != b for a, b in zip(A, B))` to compare characters directly.  \n",
        "   - **Rationale**: Simple, Pythonic, and avoids NumPy for string-specific task.\n",
        "\n",
        "#### Testing\n",
        "- **Binary Strings**: `\"1010\"` vs. `\"1100\"`  \n",
        "  - Pairs: `'1'='1'`, `'0'≠'1'`, `'1'≠'0'`, `'0'='0'` → Distance = 2 (✓).  \n",
        "- **Identical**: `\"111\"` vs. `\"111\"` → Distance = 0 (✓).  \n",
        "- **Unequal Lengths**: `\"10\"` vs. `\"101\"` → Assertion fails (✓).  \n",
        "- **Non-Binary**: `\"102\"` vs. `\"101\"` → Assertion fails (✓).  \n",
        "- **Conclusion**: Correctly counts mismatches for valid binary strings.\n",
        "\n",
        "#### Final Implementation\n",
        "```python\n",
        "def d_ham(A, B):\n",
        "    assert isinstance(A, str) and isinstance(B, str), \"Inputs must be strings\"\n",
        "    assert all(char in '01' for char in A), \"A must be a binary string\"\n",
        "    assert all(char in '01' for char in B), \"B must be a binary string\"\n",
        "    assert len(A) == len(B), \"Strings must have equal lengths\"\n",
        "    distance = sum(a != b for a, b in zip(A, B))\n",
        "    return distance\n",
        "```\n",
        "- **Features**:  \n",
        "  - Accepts binary strings (e.g., `\"101\"`, `\"110\"`).  \n",
        "  - Validates string type and binary content.  \n",
        "  - Computes distance as the number of differing positions.  \n",
        "- **Design Choices**:  \n",
        "  - Dropped NumPy for simplicity, as string operations didn’t need array overhead.  \n",
        "  - Used `zip` for character-by-character comparison.\n",
        "\n",
        "#### Alternatives Considered\n",
        "- **NumPy Version**: Convert strings to arrays (e.g., `np.array(list(A))`) and use `np.sum(A != B)`.  \n",
        "  - **Reason Not Chosen**: Less intuitive for strings, added complexity without benefit.  \n",
        "- **Flexible Inputs**: Allow non-string inputs (e.g., lists) with conversion.  \n",
        "  - **Reason Not Chosen**: Your focus was strictly binary strings.\n",
        "\n",
        "#### Open Questions (Resolved)\n",
        "1. **NumPy or Not?**: Opted for pure Python to match string focus.  \n",
        "2. **Validation Strictness?**: Enforced binary strings with assertions, aligning with your intent.\n",
        "\n",
        "#### Next Steps (If Revisited)\n",
        "- Add docstring for clarity (e.g., “Computes Hamming distance between two binary strings”).  \n",
        "- Test with larger binary strings or edge cases (e.g., empty strings, if defined).  \n",
        "- Integrate into your project alongside `d_cos`.\n",
        "\n",
        "#### Notes\n",
        "Your grasp of Hamming distance as a positional difference metric was excellent from the start. Refining it for binary strings sharpened its purpose, making it a precise tool for your project. This is ready to use or adapt as needed—great work!\n",
        "\n",
        "---\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "w4X6iPD2LOMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def d_ham(A,B):\n",
        "    assert isinstance(A, str) and isinstance(B, str), \"Inputs must be strings\"\n",
        "    # Check if both are binary (only 0s and 1s)\n",
        "    assert all(char in '01' for char in A), \"A must be a binary string\"\n",
        "    assert all(char in '01' for char in B), \"B must be a binary string\"\n",
        "    # Check equal lengths\n",
        "    assert len(A) == len(B), \"Strings must have equal lengths\"\n",
        "    # Count differences\n",
        "    distance = sum(a != b for a, b in zip(A, B))\n",
        "    return distance"
      ],
      "metadata": {
        "id": "9brmkYOyLOuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II"
      ],
      "metadata": {
        "id": "cQt-cigpRspt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = \"11010\"\n",
        "B = \"00011\"\n",
        "print(d_ham(A,B))\n",
        "#print(d_manh(A,B))\n",
        "#print(d_euc(A,B))\n",
        "#print(d_cos(A,B))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCKj3A_70-QT",
        "outputId": "0b5287f2-f422-47e7-8f46-93361c3c9373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A= \"1234\"\n",
        "print(list(A))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg_jV2uKPDhg",
        "outputId": "d8a1f085-1315-4735-f449-edd69189f919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4']\n"
          ]
        }
      ]
    }
  ]
}